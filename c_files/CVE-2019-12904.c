_gcry_aes_ctr_enc (void *context, unsigned char *ctr,
                   void *outbuf_arg, const void *inbuf_arg,
                   size_t nblocks)
{
  RIJNDAEL_context *ctx = context;
  unsigned char *outbuf = outbuf_arg;
  const unsigned char *inbuf = inbuf_arg;
  unsigned int burn_depth = 0;

  if (0)
    ;
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    {
      _gcry_aes_aesni_ctr_enc (ctx, ctr, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_AESNI*/
#ifdef USE_SSSE3
  else if (ctx->use_ssse3)
    {
      _gcry_aes_ssse3_ctr_enc (ctx, ctr, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_SSSE3*/
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    {
      _gcry_aes_armv8_ce_ctr_enc (ctx, ctr, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_ARM_CE*/
  else
    {
      union { unsigned char x1[16] ATTR_ALIGNED_16; u32 x32[4]; } tmp;
      rijndael_cryptfn_t encrypt_fn = ctx->encrypt_fn;

      if (ctx->prefetch_enc_fn)
        ctx->prefetch_enc_fn();

      for ( ;nblocks; nblocks-- )
        {
          /* Encrypt the counter. */
          burn_depth = encrypt_fn (ctx, tmp.x1, ctr);
          /* XOR the input with the encrypted counter and store in output.  */
          cipher_block_xor(outbuf, tmp.x1, inbuf, BLOCKSIZE);
          outbuf += BLOCKSIZE;
          inbuf  += BLOCKSIZE;
          /* Increment the counter.  */
	  cipher_block_add(ctr, 1, BLOCKSIZE);
        }

      wipememory(&tmp, sizeof(tmp));
    }

  if (burn_depth)
    _gcry_burn_stack (burn_depth + 4 * sizeof(void *));
}

_gcry_cipher_gcm_setkey (gcry_cipher_hd_t c)
{
  memset (c->u_mode.gcm.u_ghash_key.key, 0, GCRY_GCM_BLOCK_LEN);

  c->spec->encrypt (&c->context.c, c->u_mode.gcm.u_ghash_key.key,
                    c->u_mode.gcm.u_ghash_key.key);
  setupM (c);
}

gcm_check_aadlen_or_ivlen (u32 ctr[2])
{
  /* len(aad/iv) <= 2^64-1 bits ~= 2^61-1 bytes */
  if (ctr[1] > 0x1fffffffU)
    return 0;
  if (ctr[1] < 0x1fffffffU)
    return 1;

  if (ctr[0] <= 0xffffffffU)
    return 1;

  return 0;
}

bshift (u32 * M, int i)
{
  u32 t[4], mask;

  t[0] = M[i * 4 + 0];
  t[1] = M[i * 4 + 1];
  t[2] = M[i * 4 + 2];
  t[3] = M[i * 4 + 3];
  mask = -(t[3] & 1) & 0xe1;

  M[i * 4 + 3] = (t[3] >> 1) ^ (t[2] << 31);
  M[i * 4 + 2] = (t[2] >> 1) ^ (t[1] << 31);
  M[i * 4 + 1] = (t[1] >> 1) ^ (t[0] << 31);
  M[i * 4 + 0] = (t[0] >> 1) ^ (mask << 24);
}

_gcry_aes_cbc_enc (void *context, unsigned char *iv,
                   void *outbuf_arg, const void *inbuf_arg,
                   size_t nblocks, int cbc_mac)
{
  RIJNDAEL_context *ctx = context;
  unsigned char *outbuf = outbuf_arg;
  const unsigned char *inbuf = inbuf_arg;
  unsigned char *last_iv;
  unsigned int burn_depth = 0;

  if (0)
    ;
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    {
      _gcry_aes_aesni_cbc_enc (ctx, iv, outbuf, inbuf, nblocks, cbc_mac);
      return;
    }
#endif /*USE_AESNI*/
#ifdef USE_SSSE3
  else if (ctx->use_ssse3)
    {
      _gcry_aes_ssse3_cbc_enc (ctx, iv, outbuf, inbuf, nblocks, cbc_mac);
      return;
    }
#endif /*USE_SSSE3*/
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    {
      _gcry_aes_armv8_ce_cbc_enc (ctx, iv, outbuf, inbuf, nblocks, cbc_mac);
      return;
    }
#endif /*USE_ARM_CE*/
  else
    {
      rijndael_cryptfn_t encrypt_fn = ctx->encrypt_fn;

      if (ctx->prefetch_enc_fn)
        ctx->prefetch_enc_fn();

      last_iv = iv;

      for ( ;nblocks; nblocks-- )
        {
          cipher_block_xor(outbuf, inbuf, last_iv, BLOCKSIZE);

          burn_depth = encrypt_fn (ctx, outbuf, outbuf);

          last_iv = outbuf;
          inbuf += BLOCKSIZE;
          if (!cbc_mac)
            outbuf += BLOCKSIZE;
        }

      if (last_iv != iv)
        cipher_block_cpy (iv, last_iv, BLOCKSIZE);
    }

  if (burn_depth)
    _gcry_burn_stack (burn_depth + 4 * sizeof(void *));
}

_gcry_cipher_gcm_get_tag (gcry_cipher_hd_t c, unsigned char *outtag,
                          size_t taglen)
{
  /* Outputting authentication tag is part of encryption. */
  if (c->u_mode.gcm.disallow_encryption_because_of_setiv_in_fips_mode)
    return GPG_ERR_INV_STATE;

  return _gcry_cipher_gcm_tag (c, outtag, taglen, 0);
}

selftest_fips_128_38a (int requested_mode)
{
  static const struct tv
  {
    int mode;
    const unsigned char key[16];
    const unsigned char iv[16];
    struct
    {
      const unsigned char input[16];
      const unsigned char output[16];
    } data[4];
  } tv[2] =
    {
      {
        GCRY_CIPHER_MODE_CFB,  /* F.3.13, CFB128-AES128 */
        { 0x2b, 0x7e, 0x15, 0x16, 0x28, 0xae, 0xd2, 0xa6,
          0xab, 0xf7, 0x15, 0x88, 0x09, 0xcf, 0x4f, 0x3c },
        { 0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,
          0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f },
        {
          { { 0x6b, 0xc1, 0xbe, 0xe2, 0x2e, 0x40, 0x9f, 0x96,
              0xe9, 0x3d, 0x7e, 0x11, 0x73, 0x93, 0x17, 0x2a },
            { 0x3b, 0x3f, 0xd9, 0x2e, 0xb7, 0x2d, 0xad, 0x20,
              0x33, 0x34, 0x49, 0xf8, 0xe8, 0x3c, 0xfb, 0x4a } },

          { { 0xae, 0x2d, 0x8a, 0x57, 0x1e, 0x03, 0xac, 0x9c,
              0x9e, 0xb7, 0x6f, 0xac, 0x45, 0xaf, 0x8e, 0x51 },
            { 0xc8, 0xa6, 0x45, 0x37, 0xa0, 0xb3, 0xa9, 0x3f,
              0xcd, 0xe3, 0xcd, 0xad, 0x9f, 0x1c, 0xe5, 0x8b } },

          { { 0x30, 0xc8, 0x1c, 0x46, 0xa3, 0x5c, 0xe4, 0x11,
              0xe5, 0xfb, 0xc1, 0x19, 0x1a, 0x0a, 0x52, 0xef },
            { 0x26, 0x75, 0x1f, 0x67, 0xa3, 0xcb, 0xb1, 0x40,
              0xb1, 0x80, 0x8c, 0xf1, 0x87, 0xa4, 0xf4, 0xdf } },

          { { 0xf6, 0x9f, 0x24, 0x45, 0xdf, 0x4f, 0x9b, 0x17,
              0xad, 0x2b, 0x41, 0x7b, 0xe6, 0x6c, 0x37, 0x10 },
            { 0xc0, 0x4b, 0x05, 0x35, 0x7c, 0x5d, 0x1c, 0x0e,
              0xea, 0xc4, 0xc6, 0x6f, 0x9f, 0xf7, 0xf2, 0xe6 } }
        }
      },
      {
        GCRY_CIPHER_MODE_OFB,
        { 0x2b, 0x7e, 0x15, 0x16, 0x28, 0xae, 0xd2, 0xa6,
          0xab, 0xf7, 0x15, 0x88, 0x09, 0xcf, 0x4f, 0x3c },
        { 0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,
          0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f },
        {
          { { 0x6b, 0xc1, 0xbe, 0xe2, 0x2e, 0x40, 0x9f, 0x96,
              0xe9, 0x3d, 0x7e, 0x11, 0x73, 0x93, 0x17, 0x2a },
            { 0x3b, 0x3f, 0xd9, 0x2e, 0xb7, 0x2d, 0xad, 0x20,
              0x33, 0x34, 0x49, 0xf8, 0xe8, 0x3c, 0xfb, 0x4a } },

          { { 0xae, 0x2d, 0x8a, 0x57, 0x1e, 0x03, 0xac, 0x9c,
              0x9e, 0xb7, 0x6f, 0xac, 0x45, 0xaf, 0x8e, 0x51 },
            { 0x77, 0x89, 0x50, 0x8d, 0x16, 0x91, 0x8f, 0x03,
              0xf5, 0x3c, 0x52, 0xda, 0xc5, 0x4e, 0xd8, 0x25 } },

          { { 0x30, 0xc8, 0x1c, 0x46, 0xa3, 0x5c, 0xe4, 0x11,
              0xe5, 0xfb, 0xc1, 0x19, 0x1a, 0x0a, 0x52, 0xef },
            { 0x97, 0x40, 0x05, 0x1e, 0x9c, 0x5f, 0xec, 0xf6,
              0x43, 0x44, 0xf7, 0xa8, 0x22, 0x60, 0xed, 0xcc } },

          { { 0xf6, 0x9f, 0x24, 0x45, 0xdf, 0x4f, 0x9b, 0x17,
              0xad, 0x2b, 0x41, 0x7b, 0xe6, 0x6c, 0x37, 0x10 },
            { 0x30, 0x4c, 0x65, 0x28, 0xf6, 0x59, 0xc7, 0x78,
              0x66, 0xa5, 0x10, 0xd9, 0xc1, 0xd6, 0xae, 0x5e } },
        }
      }
    };
  unsigned char scratch[16];
  gpg_error_t err;
  int tvi, idx;
  gcry_cipher_hd_t hdenc = NULL;
  gcry_cipher_hd_t hddec = NULL;

#define Fail(a) do {           \
    _gcry_cipher_close (hdenc);  \
    _gcry_cipher_close (hddec);  \
    return a;                    \
  } while (0)

  gcry_assert (sizeof tv[0].data[0].input == sizeof scratch);
  gcry_assert (sizeof tv[0].data[0].output == sizeof scratch);

  for (tvi=0; tvi < DIM (tv); tvi++)
    if (tv[tvi].mode == requested_mode)
      break;
  if (tvi == DIM (tv))
    Fail ("no test data for this mode");

  err = _gcry_cipher_open (&hdenc, GCRY_CIPHER_AES, tv[tvi].mode, 0);
  if (err)
    Fail ("open");
  err = _gcry_cipher_open (&hddec, GCRY_CIPHER_AES, tv[tvi].mode, 0);
  if (err)
    Fail ("open");
  err = _gcry_cipher_setkey (hdenc, tv[tvi].key,  sizeof tv[tvi].key);
  if (!err)
    err = _gcry_cipher_setkey (hddec, tv[tvi].key, sizeof tv[tvi].key);
  if (err)
    Fail ("set key");
  err = _gcry_cipher_setiv (hdenc, tv[tvi].iv, sizeof tv[tvi].iv);
  if (!err)
    err = _gcry_cipher_setiv (hddec, tv[tvi].iv, sizeof tv[tvi].iv);
  if (err)
    Fail ("set IV");
  for (idx=0; idx < DIM (tv[tvi].data); idx++)
    {
      err = _gcry_cipher_encrypt (hdenc, scratch, sizeof scratch,
                                  tv[tvi].data[idx].input,
                                  sizeof tv[tvi].data[idx].input);
      if (err)
        Fail ("encrypt command");
      if (memcmp (scratch, tv[tvi].data[idx].output, sizeof scratch))
        Fail ("encrypt mismatch");
      err = _gcry_cipher_decrypt (hddec, scratch, sizeof scratch,
                                  tv[tvi].data[idx].output,
                                  sizeof tv[tvi].data[idx].output);
      if (err)
        Fail ("decrypt command");
      if (memcmp (scratch, tv[tvi].data[idx].input, sizeof scratch))
        Fail ("decrypt mismatch");
    }

#undef Fail
  _gcry_cipher_close (hdenc);
  _gcry_cipher_close (hddec);
  return NULL;
}

is_tag_length_valid(size_t taglen)
{
  switch (taglen)
    {
    /* Allowed tag lengths from NIST SP 800-38D.  */
    case 128 / 8: /* GCRY_GCM_BLOCK_LEN */
    case 120 / 8:
    case 112 / 8:
    case 104 / 8:
    case 96 / 8:
    case 64 / 8:
    case 32 / 8:
      return 1;

    default:
      return 0;
    }
}

rijndael_encrypt (void *context, byte *b, const byte *a)
{
  RIJNDAEL_context *ctx = context;

  if (ctx->prefetch_enc_fn)
    ctx->prefetch_enc_fn();

  return ctx->encrypt_fn (ctx, b, a);
}

bshift (unsigned long *b)
{
  unsigned long c;
  int i;
  c = b[3] & 1;
  for (i = 3; i > 0; i--)
    {
      b[i] = (b[i] >> 1) | (b[i - 1] << 31);
    }
  b[i] >>= 1;
  return c;
}

selftest_fips_256 (int extended, selftest_report_func_t report)
{
  const char *what;
  const char *errtxt;

  (void)extended; /* No extended tests available.  */

  what = "low-level";
  errtxt = selftest_basic_256 ();
  if (errtxt)
    goto failed;

  return 0; /* Succeeded. */

 failed:
  if (report)
    report ("cipher", GCRY_CIPHER_AES256, what, errtxt);
  return GPG_ERR_SELFTEST_FAILED;
}

do_setkey (RIJNDAEL_context *ctx, const byte *key, const unsigned keylen,
           gcry_cipher_hd_t hd)
{
  static int initialized = 0;
  static const char *selftest_failed = 0;
  int rounds;
  int i,j, r, t, rconpointer = 0;
  int KC;
#if defined(USE_AESNI) || defined(USE_PADLOCK) || defined(USE_SSSE3) \
    || defined(USE_ARM_CE)
  unsigned int hwfeatures;
#endif

  (void)hd;

  /* The on-the-fly self tests are only run in non-fips mode. In fips
     mode explicit self-tests are required.  Actually the on-the-fly
     self-tests are not fully thread-safe and it might happen that a
     failed self-test won't get noticed in another thread.

     FIXME: We might want to have a central registry of succeeded
     self-tests. */
  if (!fips_mode () && !initialized)
    {
      initialized = 1;
      selftest_failed = selftest ();
      if (selftest_failed)
        log_error ("%s\n", selftest_failed );
    }
  if (selftest_failed)
    return GPG_ERR_SELFTEST_FAILED;

  if( keylen == 128/8 )
    {
      rounds = 10;
      KC = 4;
    }
  else if ( keylen == 192/8 )
    {
      rounds = 12;
      KC = 6;
    }
  else if ( keylen == 256/8 )
    {
      rounds = 14;
      KC = 8;
    }
  else
    return GPG_ERR_INV_KEYLEN;

  ctx->rounds = rounds;

#if defined(USE_AESNI) || defined(USE_PADLOCK) || defined(USE_SSSE3) \
    || defined(USE_ARM_CE)
  hwfeatures = _gcry_get_hw_features ();
#endif

  ctx->decryption_prepared = 0;
#ifdef USE_PADLOCK
  ctx->use_padlock = 0;
#endif
#ifdef USE_AESNI
  ctx->use_aesni = 0;
#endif
#ifdef USE_SSSE3
  ctx->use_ssse3 = 0;
#endif
#ifdef USE_ARM_CE
  ctx->use_arm_ce = 0;
#endif

  if (0)
    {
      ;
    }
#ifdef USE_AESNI
  else if (hwfeatures & HWF_INTEL_AESNI)
    {
      ctx->encrypt_fn = _gcry_aes_aesni_encrypt;
      ctx->decrypt_fn = _gcry_aes_aesni_decrypt;
      ctx->prefetch_enc_fn = NULL;
      ctx->prefetch_dec_fn = NULL;
      ctx->use_aesni = 1;
      ctx->use_avx = !!(hwfeatures & HWF_INTEL_AVX);
      ctx->use_avx2 = !!(hwfeatures & HWF_INTEL_AVX2);
      if (hd)
        {
          hd->bulk.cfb_enc = _gcry_aes_aesni_cfb_enc;
          hd->bulk.cfb_dec = _gcry_aes_aesni_cfb_dec;
          hd->bulk.cbc_enc = _gcry_aes_aesni_cbc_enc;
          hd->bulk.cbc_dec = _gcry_aes_aesni_cbc_dec;
          hd->bulk.ctr_enc = _gcry_aes_aesni_ctr_enc;
          hd->bulk.ocb_crypt = _gcry_aes_aesni_ocb_crypt;
          hd->bulk.ocb_auth = _gcry_aes_aesni_ocb_auth;
          hd->bulk.xts_crypt = _gcry_aes_aesni_xts_crypt;
        }
    }
#endif
#ifdef USE_PADLOCK
  else if (hwfeatures & HWF_PADLOCK_AES && keylen == 128/8)
    {
      ctx->encrypt_fn = _gcry_aes_padlock_encrypt;
      ctx->decrypt_fn = _gcry_aes_padlock_decrypt;
      ctx->prefetch_enc_fn = NULL;
      ctx->prefetch_dec_fn = NULL;
      ctx->use_padlock = 1;
      memcpy (ctx->padlockkey, key, keylen);
    }
#endif
#ifdef USE_SSSE3
  else if (hwfeatures & HWF_INTEL_SSSE3)
    {
      ctx->encrypt_fn = _gcry_aes_ssse3_encrypt;
      ctx->decrypt_fn = _gcry_aes_ssse3_decrypt;
      ctx->prefetch_enc_fn = NULL;
      ctx->prefetch_dec_fn = NULL;
      ctx->use_ssse3 = 1;
      if (hd)
        {
          hd->bulk.cfb_enc = _gcry_aes_ssse3_cfb_enc;
          hd->bulk.cfb_dec = _gcry_aes_ssse3_cfb_dec;
          hd->bulk.cbc_enc = _gcry_aes_ssse3_cbc_enc;
          hd->bulk.cbc_dec = _gcry_aes_ssse3_cbc_dec;
          hd->bulk.ctr_enc = _gcry_aes_ssse3_ctr_enc;
          hd->bulk.ocb_crypt = _gcry_aes_ssse3_ocb_crypt;
          hd->bulk.ocb_auth = _gcry_aes_ssse3_ocb_auth;
        }
    }
#endif
#ifdef USE_ARM_CE
  else if (hwfeatures & HWF_ARM_AES)
    {
      ctx->encrypt_fn = _gcry_aes_armv8_ce_encrypt;
      ctx->decrypt_fn = _gcry_aes_armv8_ce_decrypt;
      ctx->prefetch_enc_fn = NULL;
      ctx->prefetch_dec_fn = NULL;
      ctx->use_arm_ce = 1;
      if (hd)
        {
          hd->bulk.cfb_enc = _gcry_aes_armv8_ce_cfb_enc;
          hd->bulk.cfb_dec = _gcry_aes_armv8_ce_cfb_dec;
          hd->bulk.cbc_enc = _gcry_aes_armv8_ce_cbc_enc;
          hd->bulk.cbc_dec = _gcry_aes_armv8_ce_cbc_dec;
          hd->bulk.ctr_enc = _gcry_aes_armv8_ce_ctr_enc;
          hd->bulk.ocb_crypt = _gcry_aes_armv8_ce_ocb_crypt;
          hd->bulk.ocb_auth = _gcry_aes_armv8_ce_ocb_auth;
          hd->bulk.xts_crypt = _gcry_aes_armv8_ce_xts_crypt;
        }
    }
#endif
  else
    {
      ctx->encrypt_fn = do_encrypt;
      ctx->decrypt_fn = do_decrypt;
      ctx->prefetch_enc_fn = prefetch_enc;
      ctx->prefetch_dec_fn = prefetch_dec;
    }

  /* NB: We don't yet support Padlock hardware key generation.  */

  if (0)
    {
      ;
    }
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    _gcry_aes_aesni_do_setkey (ctx, key);
#endif
#ifdef USE_SSSE3
  else if (ctx->use_ssse3)
    _gcry_aes_ssse3_do_setkey (ctx, key);
#endif
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    _gcry_aes_armv8_ce_setkey (ctx, key);
#endif
  else
    {
      const byte *sbox = ((const byte *)encT) + 1;
      union
        {
          PROPERLY_ALIGNED_TYPE dummy;
          byte data[MAXKC][4];
          u32 data32[MAXKC];
        } tkk[2];
#define k      tkk[0].data
#define k_u32  tkk[0].data32
#define tk     tkk[1].data
#define tk_u32 tkk[1].data32
#define W      (ctx->keyschenc)
#define W_u32  (ctx->keyschenc32)

      prefetch_enc();

      for (i = 0; i < keylen; i++)
        {
          k[i >> 2][i & 3] = key[i];
        }

      for (j = KC-1; j >= 0; j--)
        {
          tk_u32[j] = k_u32[j];
        }
      r = 0;
      t = 0;
      /* Copy values into round key array.  */
      for (j = 0; (j < KC) && (r < rounds + 1); )
        {
          for (; (j < KC) && (t < 4); j++, t++)
            {
              W_u32[r][t] = le_bswap32(tk_u32[j]);
            }
          if (t == 4)
            {
              r++;
              t = 0;
            }
        }

      while (r < rounds + 1)
        {
          /* While not enough round key material calculated calculate
             new values.  */
          tk[0][0] ^= sbox[tk[KC-1][1] * 4];
          tk[0][1] ^= sbox[tk[KC-1][2] * 4];
          tk[0][2] ^= sbox[tk[KC-1][3] * 4];
          tk[0][3] ^= sbox[tk[KC-1][0] * 4];
          tk[0][0] ^= rcon[rconpointer++];

          if (KC != 8)
            {
              for (j = 1; j < KC; j++)
                {
                  tk_u32[j] ^= tk_u32[j-1];
                }
            }
          else
            {
              for (j = 1; j < KC/2; j++)
                {
                  tk_u32[j] ^= tk_u32[j-1];
                }
              tk[KC/2][0] ^= sbox[tk[KC/2 - 1][0] * 4];
              tk[KC/2][1] ^= sbox[tk[KC/2 - 1][1] * 4];
              tk[KC/2][2] ^= sbox[tk[KC/2 - 1][2] * 4];
              tk[KC/2][3] ^= sbox[tk[KC/2 - 1][3] * 4];
              for (j = KC/2 + 1; j < KC; j++)
                {
                  tk_u32[j] ^= tk_u32[j-1];
                }
            }

          /* Copy values into round key array.  */
          for (j = 0; (j < KC) && (r < rounds + 1); )
            {
              for (; (j < KC) && (t < 4); j++, t++)
                {
                  W_u32[r][t] = le_bswap32(tk_u32[j]);
                }
              if (t == 4)
                {
                  r++;
                  t = 0;
                }
            }
        }
#undef W
#undef tk
#undef k
#undef W_u32
#undef tk_u32
#undef k_u32
      wipememory(&tkk, sizeof(tkk));
    }

  return 0;
}

ghash_setup_armv7_neon (gcry_cipher_hd_t c)
{
  _gcry_ghash_setup_armv7_neon(c->u_mode.gcm.u_ghash_key.key);
}

do_encrypt (const RIJNDAEL_context *ctx,
unsigned char *bx, const unsigned char *ax)
{
#ifdef USE_AMD64_ASM
return _gcry_aes_amd64_encrypt_block(ctx->keyschenc, bx, ax, ctx->rounds,
				       encT);
#elif defined(USE_ARM_ASM)
  return _gcry_aes_arm_encrypt_block(ctx->keyschenc, bx, ax, ctx->rounds, encT);
#else
return do_encrypt_fn (ctx, bx, ax);
#endif /* !USE_ARM_ASM && !USE_AMD64_ASM*/
}

selftest_cbc_128 (void)
{
  const int nblocks = 8+2;
  const int blocksize = BLOCKSIZE;
  const int context_size = sizeof(RIJNDAEL_context);

  return _gcry_selftest_helper_cbc("AES", &rijndael_setkey,
           &rijndael_encrypt, &_gcry_aes_cbc_dec, nblocks, blocksize,
	   context_size);
}

selftest_fips_192 (int extended, selftest_report_func_t report)
{
  const char *what;
  const char *errtxt;

  (void)extended; /* No extended tests available.  */

  what = "low-level";
  errtxt = selftest_basic_192 ();
  if (errtxt)
    goto failed;


  return 0; /* Succeeded. */

 failed:
  if (report)
    report ("cipher", GCRY_CIPHER_AES192, what, errtxt);
  return GPG_ERR_SELFTEST_FAILED;
}

selftest_basic_192 (void)
{
  RIJNDAEL_context *ctx;
  unsigned char *ctxmem;
  unsigned char scratch[16];

  static unsigned char plaintext_192[16] =
    {
      0x76,0x77,0x74,0x75,0xF1,0xF2,0xF3,0xF4,
      0xF8,0xF9,0xE6,0xE7,0x77,0x70,0x71,0x72
    };
  static unsigned char key_192[24] =
    {
      0x04,0x05,0x06,0x07,0x09,0x0A,0x0B,0x0C,
      0x0E,0x0F,0x10,0x11,0x13,0x14,0x15,0x16,
      0x18,0x19,0x1A,0x1B,0x1D,0x1E,0x1F,0x20
    };
  static const unsigned char ciphertext_192[16] =
    {
      0x5D,0x1E,0xF2,0x0D,0xCE,0xD6,0xBC,0xBC,
      0x12,0x13,0x1A,0xC7,0xC5,0x47,0x88,0xAA
    };

  ctx = _gcry_cipher_selftest_alloc_ctx (sizeof *ctx, &ctxmem);
  if (!ctx)
    return "failed to allocate memory";
  rijndael_setkey (ctx, key_192, sizeof(key_192), NULL);
  rijndael_encrypt (ctx, scratch, plaintext_192);
  if (memcmp (scratch, ciphertext_192, sizeof (ciphertext_192)))
    {
      xfree (ctxmem);
      return "AES-192 test encryption failed.";
    }
  rijndael_decrypt (ctx, scratch, scratch);
  xfree (ctxmem);
  if (memcmp (scratch, plaintext_192, sizeof (plaintext_192)))
    return "AES-192 test decryption failed.";

  return NULL;
}

do_fillM (unsigned char *h, u64 *M)
{
  int i, j;

  M[0 + 0] = 0;
  M[0 + 16] = 0;

  M[8 + 0] = buf_get_be64 (h + 0);
  M[8 + 16] = buf_get_be64 (h + 8);

  for (i = 4; i > 0; i /= 2)
    {
      M[i + 0] = M[2 * i + 0];
      M[i + 16] = M[2 * i + 16];

      bshift (&M[i], &M[i + 16]);
    }

  for (i = 2; i < 16; i *= 2)
    for (j = 1; j < i; j++)
      {
        M[(i + j) + 0] = M[i + 0] ^ M[j + 0];
        M[(i + j) + 16] = M[i + 16] ^ M[j + 16];
      }

  for (i = 0; i < 16; i++)
    {
      M[i + 32] = (M[i + 0] >> 4) ^ ((u64) gcmR[(M[i + 16] & 0xf) << 4] << 48);
      M[i + 48] = (M[i + 16] >> 4) ^ (M[i + 0] << 60);
    }
}

_gcry_cipher_gcm_decrypt (gcry_cipher_hd_t c,
                          byte *outbuf, size_t outbuflen,
                          const byte *inbuf, size_t inbuflen)
{
  static const unsigned char zerobuf[MAX_BLOCKSIZE];
  gcry_err_code_t err;

  if (c->spec->blocksize != GCRY_GCM_BLOCK_LEN)
    return GPG_ERR_CIPHER_ALGO;
  if (outbuflen < inbuflen)
    return GPG_ERR_BUFFER_TOO_SHORT;
  if (c->u_mode.gcm.datalen_over_limits)
    return GPG_ERR_INV_LENGTH;
  if (c->marks.tag
      || c->u_mode.gcm.ghash_data_finalized
      || !c->u_mode.gcm.ghash_fn)
    return GPG_ERR_INV_STATE;

  if (!c->marks.iv)
    _gcry_cipher_gcm_setiv (c, zerobuf, GCRY_GCM_BLOCK_LEN);

  if (!c->u_mode.gcm.ghash_aad_finalized)
    {
      /* Start of decryption marks end of AAD stream. */
      do_ghash_buf(c, c->u_mode.gcm.u_tag.tag, NULL, 0, 1);
      c->u_mode.gcm.ghash_aad_finalized = 1;
    }

  gcm_bytecounter_add(c->u_mode.gcm.datalen, inbuflen);
  if (!gcm_check_datalen(c->u_mode.gcm.datalen))
    {
      c->u_mode.gcm.datalen_over_limits = 1;
      return GPG_ERR_INV_LENGTH;
    }

  while (inbuflen)
    {
      size_t currlen = inbuflen;

      /* Since checksumming is done before decryption, process input in
       * 24KiB chunks to keep data loaded in L1 cache for decryption. */
      if (currlen > 24 * 1024)
	currlen = 24 * 1024;

      do_ghash_buf(c, c->u_mode.gcm.u_tag.tag, inbuf, currlen, 0);

      err = gcm_ctr_encrypt(c, outbuf, outbuflen, inbuf, currlen);
      if (err)
	return err;

      outbuf += currlen;
      inbuf += currlen;
      outbuflen -= currlen;
      inbuflen -= currlen;
    }

  return 0;
}

_gcry_cipher_gcm_initiv (gcry_cipher_hd_t c, const byte *iv, size_t ivlen)
{
  memset (c->u_mode.gcm.aadlen, 0, sizeof(c->u_mode.gcm.aadlen));
  memset (c->u_mode.gcm.datalen, 0, sizeof(c->u_mode.gcm.datalen));
  memset (c->u_mode.gcm.u_tag.tag, 0, GCRY_GCM_BLOCK_LEN);
  c->u_mode.gcm.datalen_over_limits = 0;
  c->u_mode.gcm.ghash_data_finalized = 0;
  c->u_mode.gcm.ghash_aad_finalized = 0;

  if (ivlen == 0)
    return GPG_ERR_INV_LENGTH;

  if (ivlen != GCRY_GCM_BLOCK_LEN - 4)
    {
      u32 iv_bytes[2] = {0, 0};
      u32 bitlengths[2][2];

      if (!c->u_mode.gcm.ghash_fn)
        return GPG_ERR_INV_STATE;

      memset(c->u_ctr.ctr, 0, GCRY_GCM_BLOCK_LEN);

      gcm_bytecounter_add(iv_bytes, ivlen);
      if (!gcm_check_aadlen_or_ivlen(iv_bytes))
        {
          c->u_mode.gcm.datalen_over_limits = 1;
          return GPG_ERR_INV_LENGTH;
        }

      do_ghash_buf(c, c->u_ctr.ctr, iv, ivlen, 1);

      /* iv length, 64-bit */
      bitlengths[1][1] = be_bswap32(iv_bytes[0] << 3);
      bitlengths[1][0] = be_bswap32((iv_bytes[0] >> 29) |
                                    (iv_bytes[1] << 3));
      /* zeros, 64-bit */
      bitlengths[0][1] = 0;
      bitlengths[0][0] = 0;

      do_ghash_buf(c, c->u_ctr.ctr, (byte*)bitlengths, GCRY_GCM_BLOCK_LEN, 1);

      wipememory (iv_bytes, sizeof iv_bytes);
      wipememory (bitlengths, sizeof bitlengths);
    }
  else
    {
      /* 96-bit IV is handled differently. */
      memcpy (c->u_ctr.ctr, iv, ivlen);
      c->u_ctr.ctr[12] = c->u_ctr.ctr[13] = c->u_ctr.ctr[14] = 0;
      c->u_ctr.ctr[15] = 1;
    }

  c->spec->encrypt (&c->context.c, c->u_mode.gcm.tagiv, c->u_ctr.ctr);

  gcm_add32_be128 (c->u_ctr.ctr, 1);

  c->unused = 0;
  c->marks.iv = 1;
  c->marks.tag = 0;

  return 0;
}

selftest_basic_128 (void)
{
  RIJNDAEL_context *ctx;
  unsigned char *ctxmem;
  unsigned char scratch[16];

  /* The test vectors are from the AES supplied ones; more or less
     randomly taken from ecb_tbl.txt (I=42,81,14) */
#if 1
  static const unsigned char plaintext_128[16] =
    {
      0x01,0x4B,0xAF,0x22,0x78,0xA6,0x9D,0x33,
      0x1D,0x51,0x80,0x10,0x36,0x43,0xE9,0x9A
    };
  static const unsigned char key_128[16] =
    {
      0xE8,0xE9,0xEA,0xEB,0xED,0xEE,0xEF,0xF0,
      0xF2,0xF3,0xF4,0xF5,0xF7,0xF8,0xF9,0xFA
    };
  static const unsigned char ciphertext_128[16] =
    {
      0x67,0x43,0xC3,0xD1,0x51,0x9A,0xB4,0xF2,
      0xCD,0x9A,0x78,0xAB,0x09,0xA5,0x11,0xBD
    };
#else
  /* Test vectors from fips-197, appendix C. */
# warning debug test vectors in use
  static const unsigned char plaintext_128[16] =
    {
      0x00,0x11,0x22,0x33,0x44,0x55,0x66,0x77,
      0x88,0x99,0xaa,0xbb,0xcc,0xdd,0xee,0xff
    };
  static const unsigned char key_128[16] =
    {
      0x00,0x01,0x02,0x03,0x04,0x05,0x06,0x07,
      0x08,0x09,0x0a,0x0b,0x0c,0x0d,0x0e,0x0f
      /* 0x2b, 0x7e, 0x15, 0x16, 0x28, 0xae, 0xd2, 0xa6, */
      /* 0xab, 0xf7, 0x15, 0x88, 0x09, 0xcf, 0x4f, 0x3c */
    };
  static const unsigned char ciphertext_128[16] =
    {
      0x69,0xc4,0xe0,0xd8,0x6a,0x7b,0x04,0x30,
      0xd8,0xcd,0xb7,0x80,0x70,0xb4,0xc5,0x5a
    };
#endif

  /* Because gcc/ld can only align the CTX struct on 8 bytes on the
     stack, we need to allocate that context on the heap.  */
  ctx = _gcry_cipher_selftest_alloc_ctx (sizeof *ctx, &ctxmem);
  if (!ctx)
    return "failed to allocate memory";

  rijndael_setkey (ctx, key_128, sizeof (key_128), NULL);
  rijndael_encrypt (ctx, scratch, plaintext_128);
  if (memcmp (scratch, ciphertext_128, sizeof (ciphertext_128)))
    {
      xfree (ctxmem);
      return "AES-128 test encryption failed.";
    }
  rijndael_decrypt (ctx, scratch, scratch);
  xfree (ctxmem);
  if (memcmp (scratch, plaintext_128, sizeof (plaintext_128)))
    return "AES-128 test decryption failed.";

  return NULL;
}

do_ghash_buf(gcry_cipher_hd_t c, byte *hash, const byte *buf,
             size_t buflen, int do_padding)
{
  unsigned int blocksize = GCRY_GCM_BLOCK_LEN;
  unsigned int unused = c->u_mode.gcm.mac_unused;
  ghash_fn_t ghash_fn = c->u_mode.gcm.ghash_fn;
  size_t nblocks, n;
  unsigned int burn = 0;

  if (buflen == 0 && (unused == 0 || !do_padding))
    return;

  do
    {
      if (buflen > 0 && (buflen + unused < blocksize || unused > 0))
        {
          n = blocksize - unused;
          n = n < buflen ? n : buflen;

          buf_cpy (&c->u_mode.gcm.macbuf[unused], buf, n);

          unused += n;
          buf += n;
          buflen -= n;
        }
      if (!buflen)
        {
          if (!do_padding)
            break;

	  n = blocksize - unused;
	  if (n > 0)
	    {
	      memset (&c->u_mode.gcm.macbuf[unused], 0, n);
	      unused = blocksize;
	    }
        }

      if (unused > 0)
        {
          gcry_assert (unused == blocksize);

          /* Process one block from macbuf.  */
          burn = ghash_fn (c, hash, c->u_mode.gcm.macbuf, 1);
          unused = 0;
        }

      nblocks = buflen / blocksize;

      if (nblocks)
        {
          burn = ghash_fn (c, hash, buf, nblocks);
          buf += blocksize * nblocks;
          buflen -= blocksize * nblocks;
        }
    }
  while (buflen > 0);

  c->u_mode.gcm.mac_unused = unused;

  if (burn)
    _gcry_burn_stack (burn);
}

ghash_armv8_ce_pmull (gcry_cipher_hd_t c, byte *result, const byte *buf,
                      size_t nblocks)
{
  return _gcry_ghash_armv8_ce_pmull(c->u_mode.gcm.u_ghash_key.key, result, buf,
                                    nblocks, c->u_mode.gcm.gcm_table);
}

ghash_setup_armv8_ce_pmull (gcry_cipher_hd_t c)
{
  _gcry_ghash_setup_armv8_ce_pmull(c->u_mode.gcm.u_ghash_key.key,
                                   c->u_mode.gcm.gcm_table);
}

gcm_bytecounter_add (u32 ctr[2], size_t add)
{
  if (sizeof(add) > sizeof(u32))
    {
      u32 high_add = ((add >> 31) >> 1) & 0xffffffff;
      ctr[1] += high_add;
    }

  ctr[0] += add;
  if (ctr[0] >= add)
    return;
  ++ctr[1];
}

selftest_basic_256 (void)
{
  RIJNDAEL_context *ctx;
  unsigned char *ctxmem;
  unsigned char scratch[16];

  static unsigned char plaintext_256[16] =
    {
      0x06,0x9A,0x00,0x7F,0xC7,0x6A,0x45,0x9F,
      0x98,0xBA,0xF9,0x17,0xFE,0xDF,0x95,0x21
    };
  static unsigned char key_256[32] =
    {
      0x08,0x09,0x0A,0x0B,0x0D,0x0E,0x0F,0x10,
      0x12,0x13,0x14,0x15,0x17,0x18,0x19,0x1A,
      0x1C,0x1D,0x1E,0x1F,0x21,0x22,0x23,0x24,
      0x26,0x27,0x28,0x29,0x2B,0x2C,0x2D,0x2E
    };
  static const unsigned char ciphertext_256[16] =
    {
      0x08,0x0E,0x95,0x17,0xEB,0x16,0x77,0x71,
      0x9A,0xCF,0x72,0x80,0x86,0x04,0x0A,0xE3
    };

  ctx = _gcry_cipher_selftest_alloc_ctx (sizeof *ctx, &ctxmem);
  if (!ctx)
    return "failed to allocate memory";
  rijndael_setkey (ctx, key_256, sizeof(key_256), NULL);
  rijndael_encrypt (ctx, scratch, plaintext_256);
  if (memcmp (scratch, ciphertext_256, sizeof (ciphertext_256)))
    {
      xfree (ctxmem);
      return "AES-256 test encryption failed.";
    }
  rijndael_decrypt (ctx, scratch, scratch);
  xfree (ctxmem);
  if (memcmp (scratch, plaintext_256, sizeof (plaintext_256)))
    return "AES-256 test decryption failed.";

  return NULL;
}

run_selftests (int algo, int extended, selftest_report_func_t report)
{
  gpg_err_code_t ec;

  switch (algo)
    {
    case GCRY_CIPHER_AES128:
      ec = selftest_fips_128 (extended, report);
      break;
    case GCRY_CIPHER_AES192:
      ec = selftest_fips_192 (extended, report);
      break;
    case GCRY_CIPHER_AES256:
      ec = selftest_fips_256 (extended, report);
      break;
    default:
      ec = GPG_ERR_CIPHER_ALGO;
      break;

    }
  return ec;
}

_gcry_aes_cfb_enc (void *context, unsigned char *iv,
                   void *outbuf_arg, const void *inbuf_arg,
                   size_t nblocks)
{
  RIJNDAEL_context *ctx = context;
  unsigned char *outbuf = outbuf_arg;
  const unsigned char *inbuf = inbuf_arg;
  unsigned int burn_depth = 0;

  if (0)
    ;
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    {
      _gcry_aes_aesni_cfb_enc (ctx, iv, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_AESNI*/
#ifdef USE_SSSE3
  else if (ctx->use_ssse3)
    {
      _gcry_aes_ssse3_cfb_enc (ctx, iv, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_SSSE3*/
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    {
      _gcry_aes_armv8_ce_cfb_enc (ctx, iv, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_ARM_CE*/
  else
    {
      rijndael_cryptfn_t encrypt_fn = ctx->encrypt_fn;

      if (ctx->prefetch_enc_fn)
        ctx->prefetch_enc_fn();

      for ( ;nblocks; nblocks-- )
        {
          /* Encrypt the IV. */
          burn_depth = encrypt_fn (ctx, iv, iv);
          /* XOR the input with the IV and store input into IV.  */
          cipher_block_xor_2dst(outbuf, iv, inbuf, BLOCKSIZE);
          outbuf += BLOCKSIZE;
          inbuf  += BLOCKSIZE;
        }
    }

  if (burn_depth)
    _gcry_burn_stack (burn_depth + 4 * sizeof(void *));
}

_gcry_aes_xts_crypt (void *context, unsigned char *tweak,
		     void *outbuf_arg, const void *inbuf_arg,
		     size_t nblocks, int encrypt)
{
  RIJNDAEL_context *ctx = context;
  unsigned char *outbuf = outbuf_arg;
  const unsigned char *inbuf = inbuf_arg;
  unsigned int burn_depth = 0;
  rijndael_cryptfn_t crypt_fn;
  u64 tweak_lo, tweak_hi, tweak_next_lo, tweak_next_hi, tmp_lo, tmp_hi, carry;

  if (0)
    ;
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    {
      _gcry_aes_aesni_xts_crypt (ctx, tweak, outbuf, inbuf, nblocks, encrypt);
      return;
    }
#endif /*USE_AESNI*/
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    {
      _gcry_aes_armv8_ce_xts_crypt (ctx, tweak, outbuf, inbuf, nblocks, encrypt);
      return;
    }
#endif /*USE_ARM_CE*/
  else
    {
      if (encrypt)
        {
          if (ctx->prefetch_enc_fn)
            ctx->prefetch_enc_fn();

          crypt_fn = ctx->encrypt_fn;
        }
      else
        {
          check_decryption_preparation (ctx);

          if (ctx->prefetch_dec_fn)
            ctx->prefetch_dec_fn();

          crypt_fn = ctx->decrypt_fn;
        }

      tweak_next_lo = buf_get_le64 (tweak + 0);
      tweak_next_hi = buf_get_le64 (tweak + 8);

      while (nblocks)
	{
	  tweak_lo = tweak_next_lo;
	  tweak_hi = tweak_next_hi;

	  /* Xor-Encrypt/Decrypt-Xor block. */
	  tmp_lo = buf_get_le64 (inbuf + 0) ^ tweak_lo;
	  tmp_hi = buf_get_le64 (inbuf + 8) ^ tweak_hi;

	  buf_put_le64 (outbuf + 0, tmp_lo);
	  buf_put_le64 (outbuf + 8, tmp_hi);

	  /* Generate next tweak. */
	  carry = -(tweak_next_hi >> 63) & 0x87;
	  tweak_next_hi = (tweak_next_hi << 1) + (tweak_next_lo >> 63);
	  tweak_next_lo = (tweak_next_lo << 1) ^ carry;

	  burn_depth = crypt_fn (ctx, outbuf, outbuf);

	  buf_put_le64 (outbuf + 0, buf_get_le64 (outbuf + 0) ^ tweak_lo);
	  buf_put_le64 (outbuf + 8, buf_get_le64 (outbuf + 8) ^ tweak_hi);

	  outbuf += GCRY_XTS_BLOCK_LEN;
	  inbuf += GCRY_XTS_BLOCK_LEN;
	  nblocks--;
	}

      buf_put_le64 (tweak + 0, tweak_next_lo);
      buf_put_le64 (tweak + 8, tweak_next_hi);
    }

  if (burn_depth)
    _gcry_burn_stack (burn_depth + 5 * sizeof(void *));
}

ghash_armv7_neon (gcry_cipher_hd_t c, byte *result, const byte *buf,
		  size_t nblocks)
{
  return _gcry_ghash_armv7_neon(c->u_mode.gcm.u_ghash_key.key, result, buf,
				nblocks);
}

_gcry_aes_cbc_dec (void *context, unsigned char *iv,
                   void *outbuf_arg, const void *inbuf_arg,
                   size_t nblocks)
{
  RIJNDAEL_context *ctx = context;
  unsigned char *outbuf = outbuf_arg;
  const unsigned char *inbuf = inbuf_arg;
  unsigned int burn_depth = 0;

  if (0)
    ;
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    {
      _gcry_aes_aesni_cbc_dec (ctx, iv, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_AESNI*/
#ifdef USE_SSSE3
  else if (ctx->use_ssse3)
    {
      _gcry_aes_ssse3_cbc_dec (ctx, iv, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_SSSE3*/
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    {
      _gcry_aes_armv8_ce_cbc_dec (ctx, iv, outbuf, inbuf, nblocks);
      return;
    }
#endif /*USE_ARM_CE*/
  else
    {
      unsigned char savebuf[BLOCKSIZE] ATTR_ALIGNED_16;
      rijndael_cryptfn_t decrypt_fn = ctx->decrypt_fn;

      check_decryption_preparation (ctx);

      if (ctx->prefetch_dec_fn)
        ctx->prefetch_dec_fn();

      for ( ;nblocks; nblocks-- )
        {
          /* INBUF is needed later and it may be identical to OUTBUF, so store
             the intermediate result to SAVEBUF.  */

          burn_depth = decrypt_fn (ctx, savebuf, inbuf);

          cipher_block_xor_n_copy_2(outbuf, savebuf, iv, inbuf, BLOCKSIZE);
          inbuf += BLOCKSIZE;
          outbuf += BLOCKSIZE;
        }

      wipememory(savebuf, sizeof(savebuf));
    }

  if (burn_depth)
    _gcry_burn_stack (burn_depth + 4 * sizeof(void *));
}

gcm_add32_be128 (byte *ctr, unsigned int add)
{
  /* 'ctr' must be aligned to four bytes. */
  const unsigned int blocksize = GCRY_GCM_BLOCK_LEN;
  u32 *pval = (u32 *)(void *)(ctr + blocksize - sizeof(u32));
  u32 val;

  val = be_bswap32(*pval) + add;
  *pval = be_bswap32(val);

  return val; /* return result as host-endian value */
}

do_ghash (unsigned char *result, const unsigned char *buf, const u64 *gcmM)
{
  u64 V[2];
  u64 tmp[2];
  const u64 *M;
  u64 T;
  u32 A;
  int i;

  cipher_block_xor (V, result, buf, 16);
  V[0] = be_bswap64 (V[0]);
  V[1] = be_bswap64 (V[1]);

  /* First round can be manually tweaked based on fact that 'tmp' is zero. */
  M = &gcmM[(V[1] & 0xf) + 32];
  V[1] >>= 4;
  tmp[0] = M[0];
  tmp[1] = M[16];
  tmp[0] ^= gcmM[(V[1] & 0xf) + 0];
  tmp[1] ^= gcmM[(V[1] & 0xf) + 16];
  V[1] >>= 4;

  i = 6;
  while (1)
    {
      M = &gcmM[(V[1] & 0xf) + 32];
      V[1] >>= 4;

      A = tmp[1] & 0xff;
      T = tmp[0];
      tmp[0] = (T >> 8) ^ ((u64) gcmR[A] << 48) ^ gcmM[(V[1] & 0xf) + 0];
      tmp[1] = (T << 56) ^ (tmp[1] >> 8) ^ gcmM[(V[1] & 0xf) + 16];

      tmp[0] ^= M[0];
      tmp[1] ^= M[16];

      if (i == 0)
        break;

      V[1] >>= 4;
      --i;
    }

  i = 7;
  while (1)
    {
      M = &gcmM[(V[0] & 0xf) + 32];
      V[0] >>= 4;

      A = tmp[1] & 0xff;
      T = tmp[0];
      tmp[0] = (T >> 8) ^ ((u64) gcmR[A] << 48) ^ gcmM[(V[0] & 0xf) + 0];
      tmp[1] = (T << 56) ^ (tmp[1] >> 8) ^ gcmM[(V[0] & 0xf) + 16];

      tmp[0] ^= M[0];
      tmp[1] ^= M[16];

      if (i == 0)
        break;

      V[0] >>= 4;
      --i;
    }

  buf_put_be64 (result + 0, tmp[0]);
  buf_put_be64 (result + 8, tmp[1]);

  return (sizeof(V) + sizeof(T) + sizeof(tmp) +
          sizeof(int)*2 + sizeof(void*)*5);
}

selftest_fips_128 (int extended, selftest_report_func_t report)
{
  const char *what;
  const char *errtxt;

  what = "low-level";
  errtxt = selftest_basic_128 ();
  if (errtxt)
    goto failed;

  if (extended)
    {
      what = "cfb";
      errtxt = selftest_fips_128_38a (GCRY_CIPHER_MODE_CFB);
      if (errtxt)
        goto failed;

      what = "ofb";
      errtxt = selftest_fips_128_38a (GCRY_CIPHER_MODE_OFB);
      if (errtxt)
        goto failed;
    }

  return 0; /* Succeeded. */

 failed:
  if (report)
    report ("cipher", GCRY_CIPHER_AES128, what, errtxt);
  return GPG_ERR_SELFTEST_FAILED;
}

_gcry_cipher_gcm_setiv (gcry_cipher_hd_t c, const byte *iv, size_t ivlen)
{
  c->marks.iv = 0;
  c->marks.tag = 0;
  c->u_mode.gcm.disallow_encryption_because_of_setiv_in_fips_mode = 0;

  if (fips_mode ())
    {
      /* Direct invocation of GCM setiv in FIPS mode disables encryption. */
      c->u_mode.gcm.disallow_encryption_because_of_setiv_in_fips_mode = 1;
    }

  return _gcry_cipher_gcm_initiv (c, iv, ivlen);
}

void prefetch_table(const void *tab, size_t len)
{
const volatile byte *vtab = tab;
size_t i;

  for (i = 0; i < len; i += 8 * 32)
{
(void)vtab[i + 0 * 32];
(void)vtab[i + 1 * 32];
(void)vtab[i + 2 * 32];
(void)vtab[i + 3 * 32];
(void)vtab[i + 4 * 32];
(void)vtab[i + 5 * 32];
(void)vtab[i + 6 * 32];
(void)vtab[i + 7 * 32];
}

(void)vtab[len - 1];
}

do_fillM (unsigned char *h, u32 *M)
{
  int i, j;

  M[0 * 4 + 0] = 0;
  M[0 * 4 + 1] = 0;
  M[0 * 4 + 2] = 0;
  M[0 * 4 + 3] = 0;

  M[8 * 4 + 0] = buf_get_be32 (h + 0);
  M[8 * 4 + 1] = buf_get_be32 (h + 4);
  M[8 * 4 + 2] = buf_get_be32 (h + 8);
  M[8 * 4 + 3] = buf_get_be32 (h + 12);

  for (i = 4; i > 0; i /= 2)
    {
      M[i * 4 + 0] = M[2 * i * 4 + 0];
      M[i * 4 + 1] = M[2 * i * 4 + 1];
      M[i * 4 + 2] = M[2 * i * 4 + 2];
      M[i * 4 + 3] = M[2 * i * 4 + 3];

      bshift (M, i);
    }

  for (i = 2; i < 16; i *= 2)
    for (j = 1; j < i; j++)
      {
        M[(i + j) * 4 + 0] = M[i * 4 + 0] ^ M[j * 4 + 0];
        M[(i + j) * 4 + 1] = M[i * 4 + 1] ^ M[j * 4 + 1];
        M[(i + j) * 4 + 2] = M[i * 4 + 2] ^ M[j * 4 + 2];
        M[(i + j) * 4 + 3] = M[i * 4 + 3] ^ M[j * 4 + 3];
      }

  for (i = 0; i < 4 * 16; i += 4)
    {
      M[i + 0 + 64] = (M[i + 0] >> 4)
                      ^ ((u64) gcmR[(M[i + 3] << 4) & 0xf0] << 16);
      M[i + 1 + 64] = (M[i + 1] >> 4) ^ (M[i + 0] << 28);
      M[i + 2 + 64] = (M[i + 2] >> 4) ^ (M[i + 1] << 28);
      M[i + 3 + 64] = (M[i + 3] >> 4) ^ (M[i + 2] << 28);
    }
}

gcm_check_datalen (u32 ctr[2])
{
  /* len(plaintext) <= 2^39-256 bits == 2^36-32 bytes == 2^32-2 blocks */
  if (ctr[1] > 0xfU)
    return 0;
  if (ctr[1] < 0xfU)
    return 1;

  if (ctr[0] <= 0xffffffe0U)
    return 1;

  return 0;
}

check_decryption_preparation (RIJNDAEL_context *ctx)
{
  if ( !ctx->decryption_prepared )
    {
      prepare_decryption ( ctx );
      ctx->decryption_prepared = 1;
    }
}

rijndael_decrypt (void *context, byte *b, const byte *a)
{
  RIJNDAEL_context *ctx = context;

  check_decryption_preparation (ctx);

  if (ctx->prefetch_dec_fn)
    ctx->prefetch_dec_fn();

  return ctx->decrypt_fn (ctx, b, a);
}

_gcry_aes_ocb_crypt (gcry_cipher_hd_t c, void *outbuf_arg,
                     const void *inbuf_arg, size_t nblocks, int encrypt)
{
  RIJNDAEL_context *ctx = (void *)&c->context.c;
  unsigned char *outbuf = outbuf_arg;
  const unsigned char *inbuf = inbuf_arg;
  unsigned int burn_depth = 0;

  if (0)
    ;
#ifdef USE_AESNI
  else if (ctx->use_aesni)
    {
      return _gcry_aes_aesni_ocb_crypt (c, outbuf, inbuf, nblocks, encrypt);
    }
#endif /*USE_AESNI*/
#ifdef USE_SSSE3
  else if (ctx->use_ssse3)
    {
      return _gcry_aes_ssse3_ocb_crypt (c, outbuf, inbuf, nblocks, encrypt);
    }
#endif /*USE_SSSE3*/
#ifdef USE_ARM_CE
  else if (ctx->use_arm_ce)
    {
      return _gcry_aes_armv8_ce_ocb_crypt (c, outbuf, inbuf, nblocks, encrypt);
    }
#endif /*USE_ARM_CE*/
  else if (encrypt)
    {
      union { unsigned char x1[16] ATTR_ALIGNED_16; u32 x32[4]; } l_tmp;
      rijndael_cryptfn_t encrypt_fn = ctx->encrypt_fn;

      if (ctx->prefetch_enc_fn)
        ctx->prefetch_enc_fn();

      for ( ;nblocks; nblocks-- )
        {
          u64 i = ++c->u_mode.ocb.data_nblocks;
          const unsigned char *l = ocb_get_l(c, i);

          /* Offset_i = Offset_{i-1} xor L_{ntz(i)} */
          cipher_block_xor_1 (c->u_iv.iv, l, BLOCKSIZE);
          cipher_block_cpy (l_tmp.x1, inbuf, BLOCKSIZE);
          /* Checksum_i = Checksum_{i-1} xor P_i  */
          cipher_block_xor_1 (c->u_ctr.ctr, l_tmp.x1, BLOCKSIZE);
          /* C_i = Offset_i xor ENCIPHER(K, P_i xor Offset_i)  */
          cipher_block_xor_1 (l_tmp.x1, c->u_iv.iv, BLOCKSIZE);
          burn_depth = encrypt_fn (ctx, l_tmp.x1, l_tmp.x1);
          cipher_block_xor_1 (l_tmp.x1, c->u_iv.iv, BLOCKSIZE);
          cipher_block_cpy (outbuf, l_tmp.x1, BLOCKSIZE);

          inbuf += BLOCKSIZE;
          outbuf += BLOCKSIZE;
        }
    }
  else
    {
      union { unsigned char x1[16] ATTR_ALIGNED_16; u32 x32[4]; } l_tmp;
      rijndael_cryptfn_t decrypt_fn = ctx->decrypt_fn;

      check_decryption_preparation (ctx);

      if (ctx->prefetch_dec_fn)
        ctx->prefetch_dec_fn();

      for ( ;nblocks; nblocks-- )
        {
          u64 i = ++c->u_mode.ocb.data_nblocks;
          const unsigned char *l = ocb_get_l(c, i);

          /* Offset_i = Offset_{i-1} xor L_{ntz(i)} */
          cipher_block_xor_1 (c->u_iv.iv, l, BLOCKSIZE);
          cipher_block_cpy (l_tmp.x1, inbuf, BLOCKSIZE);
          /* C_i = Offset_i xor ENCIPHER(K, P_i xor Offset_i)  */
          cipher_block_xor_1 (l_tmp.x1, c->u_iv.iv, BLOCKSIZE);
          burn_depth = decrypt_fn (ctx, l_tmp.x1, l_tmp.x1);
          cipher_block_xor_1 (l_tmp.x1, c->u_iv.iv, BLOCKSIZE);
          /* Checksum_i = Checksum_{i-1} xor P_i  */
          cipher_block_xor_1 (c->u_ctr.ctr, l_tmp.x1, BLOCKSIZE);
          cipher_block_cpy (outbuf, l_tmp.x1, BLOCKSIZE);

          inbuf += BLOCKSIZE;
          outbuf += BLOCKSIZE;
        }
    }

  if (burn_depth)
    _gcry_burn_stack (burn_depth + 4 * sizeof(void *));

  return 0;
}

_gcry_cipher_gcm_tag (gcry_cipher_hd_t c,
                      byte * outbuf, size_t outbuflen, int check)
{
  if (!(is_tag_length_valid (outbuflen) || outbuflen >= GCRY_GCM_BLOCK_LEN))
    return GPG_ERR_INV_LENGTH;
  if (c->u_mode.gcm.datalen_over_limits)
    return GPG_ERR_INV_LENGTH;

  if (!c->marks.tag)
    {
      u32 bitlengths[2][2];

      if (!c->u_mode.gcm.ghash_fn)
        return GPG_ERR_INV_STATE;

      /* aad length */
      bitlengths[0][1] = be_bswap32(c->u_mode.gcm.aadlen[0] << 3);
      bitlengths[0][0] = be_bswap32((c->u_mode.gcm.aadlen[0] >> 29) |
                                    (c->u_mode.gcm.aadlen[1] << 3));
      /* data length */
      bitlengths[1][1] = be_bswap32(c->u_mode.gcm.datalen[0] << 3);
      bitlengths[1][0] = be_bswap32((c->u_mode.gcm.datalen[0] >> 29) |
                                    (c->u_mode.gcm.datalen[1] << 3));

      /* Finalize data-stream. */
      do_ghash_buf(c, c->u_mode.gcm.u_tag.tag, NULL, 0, 1);
      c->u_mode.gcm.ghash_aad_finalized = 1;
      c->u_mode.gcm.ghash_data_finalized = 1;

      /* Add bitlengths to tag. */
      do_ghash_buf(c, c->u_mode.gcm.u_tag.tag, (byte*)bitlengths,
                   GCRY_GCM_BLOCK_LEN, 1);
      cipher_block_xor (c->u_mode.gcm.u_tag.tag, c->u_mode.gcm.tagiv,
                        c->u_mode.gcm.u_tag.tag, GCRY_GCM_BLOCK_LEN);
      c->marks.tag = 1;

      wipememory (bitlengths, sizeof (bitlengths));
      wipememory (c->u_mode.gcm.macbuf, GCRY_GCM_BLOCK_LEN);
      wipememory (c->u_mode.gcm.tagiv, GCRY_GCM_BLOCK_LEN);
      wipememory (c->u_mode.gcm.aadlen, sizeof (c->u_mode.gcm.aadlen));
      wipememory (c->u_mode.gcm.datalen, sizeof (c->u_mode.gcm.datalen));
    }

  if (!check)
    {
      if (outbuflen > GCRY_GCM_BLOCK_LEN)
        outbuflen = GCRY_GCM_BLOCK_LEN;

      /* NB: We already checked that OUTBUF is large enough to hold
       * the result or has valid truncated length.  */
      memcpy (outbuf, c->u_mode.gcm.u_tag.tag, outbuflen);
    }
  else
    {
      /* OUTBUFLEN gives the length of the user supplied tag in OUTBUF
       * and thus we need to compare its length first.  */
      if (!is_tag_length_valid (outbuflen)
          || !buf_eq_const (outbuf, c->u_mode.gcm.u_tag.tag, outbuflen))
        return GPG_ERR_CHECKSUM;
    }

  return 0;
}

selftest_ctr_128 (void)
{
  const int nblocks = 8+1;
  const int blocksize = BLOCKSIZE;
  const int context_size = sizeof(RIJNDAEL_context);

  return _gcry_selftest_helper_ctr("AES", &rijndael_setkey,
           &rijndael_encrypt, &_gcry_aes_ctr_enc, nblocks, blocksize,
	   context_size);
}

gcm_ctr_encrypt (gcry_cipher_hd_t c, byte *outbuf, size_t outbuflen,
                 const byte *inbuf, size_t inbuflen)
{
  gcry_err_code_t err = 0;

  while (inbuflen)
    {
      u32 nblocks_to_overflow;
      u32 num_ctr_increments;
      u32 curr_ctr_low;
      size_t currlen = inbuflen;
      byte ctr_copy[GCRY_GCM_BLOCK_LEN];
      int fix_ctr = 0;

      /* GCM CTR increments only least significant 32-bits, without carry
       * to upper 96-bits of counter.  Using generic CTR implementation
       * directly would carry 32-bit overflow to upper 96-bit.  Detect
       * if input length is long enough to cause overflow, and limit
       * input length so that CTR overflow happen but updated CTR value is
       * not used to encrypt further input.  After overflow, upper 96 bits
       * of CTR are restored to cancel out modification done by generic CTR
       * encryption. */

      if (inbuflen > c->unused)
        {
          curr_ctr_low = gcm_add32_be128 (c->u_ctr.ctr, 0);

          /* Number of CTR increments this inbuflen would cause. */
          num_ctr_increments = (inbuflen - c->unused) / GCRY_GCM_BLOCK_LEN +
                               !!((inbuflen - c->unused) % GCRY_GCM_BLOCK_LEN);

          if ((u32)(num_ctr_increments + curr_ctr_low) < curr_ctr_low)
            {
              nblocks_to_overflow = 0xffffffffU - curr_ctr_low + 1;
              currlen = nblocks_to_overflow * GCRY_GCM_BLOCK_LEN + c->unused;
              if (currlen > inbuflen)
                {
                  currlen = inbuflen;
                }

              fix_ctr = 1;
              cipher_block_cpy(ctr_copy, c->u_ctr.ctr, GCRY_GCM_BLOCK_LEN);
            }
        }

      err = _gcry_cipher_ctr_encrypt(c, outbuf, outbuflen, inbuf, currlen);
      if (err != 0)
        return err;

      if (fix_ctr)
        {
          /* Lower 32-bits of CTR should now be zero. */
          gcry_assert(gcm_add32_be128 (c->u_ctr.ctr, 0) == 0);

          /* Restore upper part of CTR. */
          buf_cpy(c->u_ctr.ctr, ctr_copy, GCRY_GCM_BLOCK_LEN - sizeof(u32));

          wipememory(ctr_copy, sizeof(ctr_copy));
        }

      inbuflen -= currlen;
      inbuf += currlen;
      outbuflen -= currlen;
      outbuf += currlen;
    }

  return err;
}
