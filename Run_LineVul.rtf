$ssh eddie.ecdf.ed.ac.uk
$qlogin
$cd /exports/eddie/scratch/username



1. Anaconda: 
Download anaconda in scratch not in home( the quota of disk is not enough)
$module load anaconda, and create a env with python3.9 in Eddie( other envs have already set if I choose python3.9)
$conda activate mypython



2.Git clone: the URL in README file is wrong

git clone https://github.com/awsm-research/LineVul.git
cd LineVul



3.requirements:
Some of them will not in the PATH if just pip install it.



4.Experiment Replication Preparation:
After download model, I use code to test whether it works:

from transformers import RobertaForSequenceClassification
def load_model():
model = RobertaForSequenceClassification.from_pretrained('distilroberta-base')
return model

if __name__ == "__main__":
load_model()

Note:
cd data
cd big-vul_dataset
gdown https://drive.google.com/uc?id=1ldXyFvHG41VMrm260cK_JEPYqeb6e6Yw
gdown https://drive.google.com/uc?id=1yggncqivMcP0tzbh8-8Eu02Edwcs44WZ
cd ../..
In upper code, the first down is not available, I download it on my pc and create a new URL for gdown:

FILE_ID=1ldXyFvHG41VMrm260cK_JEPYqeb6e6Yw
FILE_NAME=train.csv

wget --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILE_ID -O $FILE_NAME

5.Replication:

A. RQ1

a. First we need to use cuda(GPU), use the code below:
1. qlogin -q gpu -pe gpu-a100 1 -l h_rt=12:00:00 -l h_vmem=16G
2. or qrsh -l h_rt=12:00:00 -l h_vmem=32G -q gpu -pe gpu-a100 2
3. or create a run_gpu.sh file and then qsub:
nano run_gpu.sh
chmod +x run_gpu.sh
qsub run_gpu.sh   

If choose first two, and it works, then 
$module load cuda
$source /exports/applications/support/set_qlogin_environment.sh
Note: 
1. 8 hours is not enough for rq1
2. Parallel is not working, so only use one GPU

run_gpu.sh:

#!/bin/bash

# Grid Engine options (lines prefixed with #$)
# Runtime limit of 12 hour:
#$ -l h_rt=12:00:00
#
# Set working directory to the directory where the job is submitted from:
#$ -cwd
#
# Request one GPU in the gpu queue:
#$ -q gpu
#$ -pe gpu-a100 1
#
# Request 16 GB system RAM
# the total system RAM available to the job is the value specified here multipl$
# the number of requested GPUs (above)
#$ -l h_vmem=16G

# Initialise the environment modules and load CUDA version 11.0.2
. /etc/profile.d/modules.sh
module load cuda
source /exports/applications/support/set_qlogin_environment.sh
# Navigate to the desired directory
cd /exports/eddie/scratch/username/LineVul

module load anaconda
conda activate mypython

b. #To retrain the RQ1 model, run the following commands (Training + Inference):
cd linevul
python linevul_main.py \
  --output_dir=./saved_models \
  --model_type=roberta \
  --tokenizer_name=microsoft/codebert-base \
  --model_name_or_path=microsoft/codebert-base \
  --do_train \
  --do_test \
  --train_data_file=../data/big-vul_dataset/train.csv \
  --eval_data_file=../data/big-vul_dataset/val.csv \
  --test_data_file=../data/big-vul_dataset/test.csv \
  --epochs 10 \
  --block_size 512 \
  --train_batch_size 16 \
  --eval_batch_size 16 \
  --learning_rate 2e-5 \
  --max_grad_norm 1.0 \
  --evaluate_during_training \
  --seed 123456  2>&1 | tee train.log

c. #To reproduce the RQ1 result of BoW+RF, run the following commands:
cd ..
cd bow_rf
mkdir saved_models
python rf_main.py

Note: Still need GPU, or will report error “storage is not enough”

B. RQ2

cd LineVul
cd linevul
python linevul_main.py \
  --model_name=12heads_linevul_model.bin \
  --output_dir=./saved_models \
  --model_type=roberta \
  --tokenizer_name=microsoft/codebert-base \
  --model_name_or_path=microsoft/codebert-base \
  --do_test \
  --do_local_explanation \
  --top_k_constant=10 \
  --reasoning_method=all \
  --train_data_file=../data/big-vul_dataset/train.csv \
  --eval_data_file=../data/big-vul_dataset/val.csv \
  --test_data_file=../data/big-vul_dataset/test.csv \
  --block_size 512 \
  --eval_batch_size 512

cd cppcheck
python run.py

Note: 
If want use cppcheck: 
1. sudo apt-get install cppcheck
2. if not have root
mkdir -p $HOME/cppcheck/bin
cp bin/cppcheck $HOME/cppcheck/bin
echo 'export PATH=$HOME/cppcheck/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
cppcheck --version

C. RQ3

#LineVul
(mypython) [s2291592@node3c01(eddie) linevul]$ python linevul_main.py \
>   --model_name=12heads_linevul_model.bin \
>   --output_dir=./saved_models \
>   --model_type=roberta \
>   --tokenizer_name=microsoft/codebert-base \
>   --model_name_or_path=microsoft/codebert-base \
>   --do_test \
>   --train_data_file=../data/big-vul_dataset/train.csv \
>   --eval_data_file=../data/big-vul_dataset/val.csv \
>   --test_data_file=../data/big-vul_dataset/test.csv \
>   --block_size 512 \
>   --eval_batch_size 512
07/03/2024 15:49:31 - WARNING - __main__ -   device: cuda, n_gpu: 1
/exports/eddie/scratch/s2291592/anaconda/envs/mypython/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
07/03/2024 15:49:36 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../data/big-vul_dataset/train.csv', output_dir='./saved_models', model_type='roberta', block_size=512, eval_data_file='../data/big-vul_dataset/val.csv', test_data_file='../data/big-vul_dataset/test.csv', model_name='12heads_linevul_model.bin', model_name_or_path='microsoft/codebert-base', config_name='', use_non_pretrained_model=False, tokenizer_name='microsoft/codebert-base', code_length=256, do_train=False, do_eval=False, do_test=True, evaluate_during_training=False, do_local_explanation=False, reasoning_method=None, train_batch_size=4, eval_batch_size=512, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, warmup_steps=0, seed=42, epochs=1, effort_at_top_k=0.2, top_k_recall_by_lines=0.01, top_k_recall_by_pred_prob=0.2, do_sorting_by_line_scores=False, do_sorting_by_pred_prob=False, top_k_constant=10, num_attention_heads=12, write_raw_preds=False, use_word_level_tokenizer=False, use_non_pretrained_tokenizer=False, n_gpu=1, device=device(type='cuda'))
100%|████████████████████████████████████| 18864/18864 [00:23<00:00, 807.57it/s]
07/03/2024 15:50:08 - INFO - __main__ -   ***** Running Test *****
07/03/2024 15:50:08 - INFO - __main__ -     Num examples = 18864
07/03/2024 15:50:08 - INFO - __main__ -     Batch size = 512
07/03/2024 15:52:15 - INFO - __main__ -   ***** Test results *****
07/03/2024 15:52:15 - INFO - __main__ -     test_accuracy = 0.9909
07/03/2024 15:52:15 - INFO - __main__ -     test_f1 = 0.9142
07/03/2024 15:52:15 - INFO - __main__ -     test_precision = 0.9712
07/03/2024 15:52:15 - INFO - __main__ -     test_recall = 0.8635
07/03/2024 15:52:15 - INFO - __main__ -     test_threshold = 0.5



#BPE+No Pretraining+BERT
(mypython) [s2291592@node3c01(eddie) linevul]$ python linevul_main.py \
>   --model_name=bpebert.bin \
>   --output_dir=./saved_models \
>   --model_type=roberta \
>   --tokenizer_name=microsoft/codebert-base \
>   --model_name_or_path=microsoft/codebert-base \
>   --do_test \
>   --train_data_file=../data/big-vul_dataset/train.csv \
>   --eval_data_file=../data/big-vul_dataset/val.csv \
>   --test_data_file=../data/big-vul_dataset/test.csv \
>   --block_size 512 \
>   --eval_batch_size 512
07/03/2024 15:54:53 - WARNING - __main__ -   device: cuda, n_gpu: 1
/exports/eddie/scratch/s2291592/anaconda/envs/mypython/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
07/03/2024 15:54:58 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../data/big-vul_dataset/train.csv', output_dir='./saved_models', model_type='roberta', block_size=512, eval_data_file='../data/big-vul_dataset/val.csv', test_data_file='../data/big-vul_dataset/test.csv', model_name='bpebert.bin', model_name_or_path='microsoft/codebert-base', config_name='', use_non_pretrained_model=False, tokenizer_name='microsoft/codebert-base', code_length=256, do_train=False, do_eval=False, do_test=True, evaluate_during_training=False, do_local_explanation=False, reasoning_method=None, train_batch_size=4, eval_batch_size=512, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, warmup_steps=0, seed=42, epochs=1, effort_at_top_k=0.2, top_k_recall_by_lines=0.01, top_k_recall_by_pred_prob=0.2, do_sorting_by_line_scores=False, do_sorting_by_pred_prob=False, top_k_constant=10, num_attention_heads=12, write_raw_preds=False, use_word_level_tokenizer=False, use_non_pretrained_tokenizer=False, n_gpu=1, device=device(type='cuda'))
100%|████████████████████████████████████| 18864/18864 [00:23<00:00, 810.83it/s]
07/03/2024 15:55:30 - INFO - __main__ -   ***** Running Test *****
07/03/2024 15:55:30 - INFO - __main__ -     Num examples = 18864
07/03/2024 15:55:30 - INFO - __main__ -     Batch size = 512
07/03/2024 15:57:37 - INFO - __main__ -   ***** Test results *****
07/03/2024 15:57:37 - INFO - __main__ -     test_accuracy = 0.8087
07/03/2024 15:57:37 - INFO - __main__ -     test_f1 = 0.0758
07/03/2024 15:57:37 - INFO - __main__ -     test_precision = 0.0519
07/03/2024 15:57:37 - INFO - __main__ -     test_recall = 0.1403
07/03/2024 15:57:37 - INFO - __main__ -     test_threshold = 0.5



